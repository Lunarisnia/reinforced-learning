{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-15T10:01:29.074202500Z",
     "start_time": "2024-02-15T10:01:29.062298300Z"
    }
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.monitor import Monitor"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Create our environment we want LunarLander-v2\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "\n",
    "# Reset the environment to its original state\n",
    "observation, info = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    # Take random action\n",
    "    action = env.action_space.sample()\n",
    "    print(f\"Action Taken: {action}\")\n",
    "    \n",
    "    # Do this action in the environment and get\n",
    "    # next_state, reward, terminated, truncated, and info\n",
    "    observation, reward, terminated, truncated, info = env.step(action)\n",
    "    \n",
    "    # If the game is terminated (in our case we land, crashed) or truncated (timeout)\n",
    "    if terminated or truncated:\n",
    "        # Reset the environment\n",
    "        print(\"Environment is reset\")\n",
    "        observation, info = env.reset()\n",
    "        \n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f01895d853f03c40",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "Output()",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cd9b6e2476d84276a2c8592a104b1707"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[10], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m env \u001B[38;5;241m=\u001B[39m make_vec_env(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mLunarLander-v2\u001B[39m\u001B[38;5;124m\"\u001B[39m, n_envs\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m32\u001B[39m)\n\u001B[0;32m      2\u001B[0m model \u001B[38;5;241m=\u001B[39m PPO(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMlpPolicy\u001B[39m\u001B[38;5;124m\"\u001B[39m, env, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m----> 3\u001B[0m model\u001B[38;5;241m.\u001B[39mlearn(total_timesteps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3e6\u001B[39m, progress_bar\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m      4\u001B[0m model_name \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mppo-lunar-landerv2-v2-3e6-lr3e-2\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m      5\u001B[0m model\u001B[38;5;241m.\u001B[39msave(model_name)\n",
      "File \u001B[1;32m~\\.conda\\envs\\pgt\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001B[0m, in \u001B[0;36mPPO.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    306\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlearn\u001B[39m(\n\u001B[0;32m    307\u001B[0m     \u001B[38;5;28mself\u001B[39m: SelfPPO,\n\u001B[0;32m    308\u001B[0m     total_timesteps: \u001B[38;5;28mint\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    313\u001B[0m     progress_bar: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    314\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m SelfPPO:\n\u001B[1;32m--> 315\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mlearn(\n\u001B[0;32m    316\u001B[0m         total_timesteps\u001B[38;5;241m=\u001B[39mtotal_timesteps,\n\u001B[0;32m    317\u001B[0m         callback\u001B[38;5;241m=\u001B[39mcallback,\n\u001B[0;32m    318\u001B[0m         log_interval\u001B[38;5;241m=\u001B[39mlog_interval,\n\u001B[0;32m    319\u001B[0m         tb_log_name\u001B[38;5;241m=\u001B[39mtb_log_name,\n\u001B[0;32m    320\u001B[0m         reset_num_timesteps\u001B[38;5;241m=\u001B[39mreset_num_timesteps,\n\u001B[0;32m    321\u001B[0m         progress_bar\u001B[38;5;241m=\u001B[39mprogress_bar,\n\u001B[0;32m    322\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\pgt\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:277\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.learn\u001B[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001B[0m\n\u001B[0;32m    274\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    276\u001B[0m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m<\u001B[39m total_timesteps:\n\u001B[1;32m--> 277\u001B[0m     continue_training \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcollect_rollouts(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39menv, callback, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrollout_buffer, n_rollout_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mn_steps)\n\u001B[0;32m    279\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m continue_training:\n\u001B[0;32m    280\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\pgt\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:199\u001B[0m, in \u001B[0;36mOnPolicyAlgorithm.collect_rollouts\u001B[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001B[0m\n\u001B[0;32m    196\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_timesteps \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m env\u001B[38;5;241m.\u001B[39mnum_envs\n\u001B[0;32m    198\u001B[0m \u001B[38;5;66;03m# Give access to local variables\u001B[39;00m\n\u001B[1;32m--> 199\u001B[0m callback\u001B[38;5;241m.\u001B[39mupdate_locals(\u001B[38;5;28mlocals\u001B[39m())\n\u001B[0;32m    200\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m callback\u001B[38;5;241m.\u001B[39mon_step():\n\u001B[0;32m    201\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\pgt\\Lib\\site-packages\\stable_baselines3\\common\\callbacks.py:134\u001B[0m, in \u001B[0;36mBaseCallback.update_locals\u001B[1;34m(self, locals_)\u001B[0m\n\u001B[0;32m    128\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate_locals\u001B[39m(\u001B[38;5;28mself\u001B[39m, locals_: Dict[\u001B[38;5;28mstr\u001B[39m, Any]) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    129\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    130\u001B[0m \u001B[38;5;124;03m    Update the references to the local variables.\u001B[39;00m\n\u001B[0;32m    131\u001B[0m \n\u001B[0;32m    132\u001B[0m \u001B[38;5;124;03m    :param locals_: the local variables during rollout collection\u001B[39;00m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 134\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlocals\u001B[38;5;241m.\u001B[39mupdate(locals_)\n\u001B[0;32m    135\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate_child_locals(locals_)\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "env = make_vec_env(\"LunarLander-v2\", n_envs=32)\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=3e6, progress_bar=True)\n",
    "model_name = \"ppo-lunar-landerv2-v2-3e6-lr3e-2\"\n",
    "model.save(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T04:58:08.750413600Z",
     "start_time": "2024-02-15T04:58:02.778394200Z"
    }
   },
   "id": "fc64a59de91b6618",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward=263.77 +/- 17.134602775684165\n"
     ]
    }
   ],
   "source": [
    "eval_env = Monitor(gym.make(\"LunarLander-v2\"))\n",
    "model = PPO.load(\"./ppo-lunar-landerv2-v1-5e6-lr3e-4.zip\", env, device=\"cuda\")\n",
    "mean_reward, std_reward = evaluate_policy(model, eval_env, n_eval_episodes=10, deterministic=True)\n",
    "print(f\"mean_reward={mean_reward:.2f} +/- {std_reward}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T10:01:36.065846700Z",
     "start_time": "2024-02-15T10:01:34.133944500Z"
    }
   },
   "id": "ed6106096cb702c",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# How to make predictions\n",
    "env = make_vec_env(\"LunarLander-v2\")\n",
    "model = PPO.load(\"./ppo-lunar-landerv2-v1.zip\", env, device=\"cuda\")\n",
    "\n",
    "observation = env.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    action = model.predict(observation, deterministic=True)\n",
    "    \n",
    "    observation, reward, terminated, truncated = env.step(action[0])\n",
    "    env.render(\"human\")\n",
    "\n",
    "env.close()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4ac0c0a4ba3b8b54",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;4mℹ This function will save, evaluate, generate a video of your agent,\n",
      "create a model card and push everything to the hub. It might take up to 1min.\n",
      "This is a work in progress: if you encounter a bug, please open an issue.\u001B[0m\n",
      "Saving video to C:\\Users\\Loona\\AppData\\Local\\Temp\\tmp0vnfiimc\\-step-0-to-step-1000.mp4\n",
      "Moviepy - Building video C:\\Users\\Loona\\AppData\\Local\\Temp\\tmp0vnfiimc\\-step-0-to-step-1000.mp4.\n",
      "Moviepy - Writing video C:\\Users\\Loona\\AppData\\Local\\Temp\\tmp0vnfiimc\\-step-0-to-step-1000.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready C:\\Users\\Loona\\AppData\\Local\\Temp\\tmp0vnfiimc\\-step-0-to-step-1000.mp4\n",
      "\u001B[38;5;4mℹ Pushing repo Lounarisnia/ppo-lunar-landerv2-v1-5e6-lr3e-4 to the\n",
      "Hugging Face Hub\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "policy.optimizer.pth:   0%|          | 0.00/88.5k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "359ae5a0c7d84d07b776c718624feabd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "policy.pth:   0%|          | 0.00/43.8k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3b1f0e8308d04b3f9c527fab00aa4085"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "pytorch_variables.pth:   0%|          | 0.00/864 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1c98a43cadc54b5096eb28a3b207a547"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Upload 4 LFS files:   0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "384ce10916d2435a9a95e04d1ede4870"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "ppo-lunar-landerv2-v1-5e6-lr3e-4.zip:   0%|          | 0.00/149k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "949eba96c25e492a9ea36ac302faa1c4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[38;5;4mℹ Your model is pushed to the Hub. You can view your model here:\n",
      "https://huggingface.co/Lounarisnia/ppo-lunar-landerv2-v1-5e6-lr3e-4/tree/main/\u001B[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": "CommitInfo(commit_url='https://huggingface.co/Lounarisnia/ppo-lunar-landerv2-v1-5e6-lr3e-4/commit/9b8593306ccd09b48b8dff5941b5e269131aa67d', commit_message='Upload PPO LunarLander-v2 trained agent', commit_description='', oid='9b8593306ccd09b48b8dff5941b5e269131aa67d', pr_url=None, pr_revision=None, pr_num=None)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "from huggingface_sb3 import package_to_hub\n",
    "\n",
    "model_name = \"ppo-lunar-landerv2-v1-5e6-lr3e-4\"\n",
    "env = make_vec_env(\"LunarLander-v2\")\n",
    "model = PPO.load(\"./ppo-lunar-landerv2-v1-5e6-lr3e-4.zip\", env, device=\"cuda\")\n",
    "\n",
    "# PLACE the variables you've just defined two cells above\n",
    "# Define the name of the environment\n",
    "env_id = \"LunarLander-v2\"\n",
    "\n",
    "# TODO: Define the model architecture we used\n",
    "model_architecture = \"PPO\"\n",
    "\n",
    "## Define a repo_id\n",
    "## repo_id is the id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "## CHANGE WITH YOUR REPO ID\n",
    "repo_id = \"Lounarisnia/ppo-lunar-landerv2-v1-5e6-lr3e-4\" # Change with your repo id, you can't push with mine 😄\n",
    "\n",
    "## Define the commit message\n",
    "commit_message = \"Upload PPO LunarLander-v2 trained agent\"\n",
    "\n",
    "# Create the evaluation env and set the render_mode=\"rgb_array\"\n",
    "eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "\n",
    "# PLACE the package_to_hub function you've just filled here\n",
    "package_to_hub(model=model, # Our trained model\n",
    "               model_name=model_name, # The name of our trained model\n",
    "               model_architecture=model_architecture, # The model architecture we used: in our case PPO\n",
    "               env_id=env_id, # Name of the environment\n",
    "               eval_env=eval_env, # Evaluation Environment\n",
    "               repo_id=repo_id, # id of the model repository from the Hugging Face Hub (repo_id = {organization}/{repo_name} for instance ThomasSimonini/ppo-LunarLander-v2\n",
    "               commit_message=commit_message)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-15T10:02:22.033336700Z",
     "start_time": "2024-02-15T10:02:07.273198500Z"
    }
   },
   "id": "688b800253437aa1",
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
