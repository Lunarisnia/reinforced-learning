{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Codex:\n",
    "0 : \"Home Page\"\n",
    "1 : \"Explore\"\n",
    "2 : \"Product A\"\n",
    "3 : \"Product B\"\n",
    "4 : \"Product C\"\n",
    "5 : \"Add Product\"\n",
    "6 : \"Close App\"\n",
    "7 : \"Buy\""
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7f4b05c1b6bb902b"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "import math\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "\n",
    "# torch.manual_seed(208)\n",
    "class VirtualShopper:\n",
    "    def __init__(self):\n",
    "        self.products = {0: 2, 1: 3, 2: 4}\n",
    "        self.product_scores = torch.tensor([0.85, 0.2, 0.1])\n",
    "        # self.product_scores = F.softmax(self.product_scores, dim=0)\n",
    "\n",
    "        # Make an environment where if the user has two or more Product C in their cart it will more likely buy\n",
    "        self.selected_products = torch.multinomial(self.product_scores, num_samples=3, replacement=True)\n",
    "        self.actions = [0, 1]\n",
    "        self.cart = []\n",
    "        for sp in self.selected_products:\n",
    "            self.actions.append(self.products[int(sp)])\n",
    "            adding_to_cart = 5 if random.randint(0, 1) == 1 else 1\n",
    "            self.actions.append(adding_to_cart)\n",
    "            if adding_to_cart == 5:\n",
    "                self.cart.append(self.products[int(sp)])\n",
    "\n",
    "        chance_of_buying = 0.\n",
    "        for item in self.cart:\n",
    "            if item == 4:\n",
    "                chance_of_buying += 3.33\n",
    "            if item == 3:\n",
    "                chance_of_buying += 1.\n",
    "            if item == 1:\n",
    "                chance_of_buying += 0.25\n",
    "\n",
    "        if chance_of_buying >= random.random() * 10:\n",
    "            self.actions.append(7)\n",
    "        else:\n",
    "            self.actions.append(6)\n",
    "\n",
    "    def get_actions(self):\n",
    "        return self.actions[:-1]\n",
    "\n",
    "    def get_labels(self):\n",
    "        return self.actions[-1] == 7\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T07:53:39.550425Z",
     "start_time": "2024-03-06T07:53:39.544658Z"
    }
   },
   "id": "fd62898aaf02d9c",
   "execution_count": 586
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# for _ in range(batch_size):\n",
    "#     action_batch = []\n",
    "#     label_batch = []\n",
    "#     for _ in range(100):\n",
    "#         user_activity = VirtualShopper()\n",
    "#         action_batch.append(torch.tensor(user_activity.get_actions(), dtype=torch.long))\n",
    "#         label_batch.append(torch.tensor([user_activity.get_labels()], dtype=torch.bool))\n",
    "#     # user_actions.append([[1, 2]])\n",
    "#     user_actions.append(action_batch)\n",
    "#     action_labels.append(label_batch)\n",
    "# \n",
    "# user_actions = torch.tensor(user_actions)\n",
    "# action_labels = torch.tensor(action_labels)\n",
    "# print(user_actions.shape)\n",
    "# print(action_labels.shape)\n",
    "# print(torch.stack(user_actions))\n",
    "\n",
    "# print(user_actions[0][:2])\n",
    "# print(action_labels[0][:2])\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_actions = []\n",
    "train_labels = []\n",
    "\n",
    "for _ in range(3000):\n",
    "    user_activity = VirtualShopper()\n",
    "    train_actions.append(torch.tensor(user_activity.get_actions(), dtype=torch.float))\n",
    "    train_labels.append(torch.tensor([user_activity.get_labels()], dtype=torch.float))\n",
    "\n",
    "val_actions = []\n",
    "val_labels = []\n",
    "for _ in range(1000):\n",
    "    user_activity = VirtualShopper()\n",
    "    val_actions.append(torch.tensor(user_activity.get_actions(), dtype=torch.float))\n",
    "    val_labels.append(torch.tensor([user_activity.get_labels()], dtype=torch.float))\n",
    "\n",
    "train_actions = torch.stack(train_actions)  # (T, C)\n",
    "train_labels = torch.stack(train_labels)  # (T, C)\n",
    "\n",
    "val_actions = torch.stack(val_actions)  # (T, C)\n",
    "val_labels = torch.stack(val_labels)  # (T, C)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T09:44:43.732235Z",
     "start_time": "2024-03-06T09:44:43.611340Z"
    }
   },
   "id": "6f38cbfaaf58254b",
   "execution_count": 1684
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "torch.manual_seed(28)\n",
    "\n",
    "class Head(nn.Module):\n",
    "    def __init__(self, head_size=3):\n",
    "        super().__init__()\n",
    "        self.query = nn.Linear(32, head_size, bias=False)\n",
    "        self.key = nn.Linear(32, head_size, bias=False)\n",
    "        self.value = nn.Linear(32, head_size, bias=False)\n",
    "        # self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "        # self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.shape\n",
    "        # Produce a query, key, value by passing it through a linear layer\n",
    "        q = self.query(x)  # (B, T, hs)\n",
    "        k = self.key(x)  # (B, T, hs)\n",
    "        v = self.value(x)  # (B, T, hs)\n",
    "\n",
    "        # do the scaled attention formula refer to attention is all you need paper\n",
    "        wei = q @ k.transpose(-2, -1) * (1 / math.sqrt(C))  # (B, T, hs) @ (B, hs, T) = (B, T, T) * (1 / sqrt(C)\n",
    "        # normalize it to every column sums up to 1\n",
    "        wei = F.softmax(wei, dim=-1)\n",
    "        # wei = self.dropout(wei)\n",
    "        # multiply the weight to the value\n",
    "        out = wei @ v  # (B, T, T) @ (B, T, hs) = (B, T, hs)\n",
    "\n",
    "        # return the result\n",
    "        return out\n",
    "\n",
    "# TODO: Rewrite the whole thing to use an Embeddings follow the paper closely or repurpose this as the FFN\n",
    "class SimpleMLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.attention_head = Head(head_size=32)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(32, 32*4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32*4, 1),\n",
    "            # nn.Softmax(dim=-1)\n",
    "        )\n",
    "        self.projection = nn.Linear(8, 32, bias=False)\n",
    "\n",
    "    def forward(self, activities, labels=None):\n",
    "        proj = self.projection(activities)\n",
    "        attention = self.attention_head(proj)\n",
    "        logits = self.mlp(attention)  # T, C\n",
    "        if labels == None:\n",
    "            loss = None\n",
    "        else:\n",
    "            loss = F.binary_cross_entropy_with_logits(logits, labels)\n",
    "        return logits, loss\n",
    "\n",
    "\n",
    "model = SimpleMLP()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:00:18.490404Z",
     "start_time": "2024-03-06T10:00:18.482195Z"
    }
   },
   "id": "9b9a1c99867d286b",
   "execution_count": 1940
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.4192, grad_fn=<BinaryCrossEntropyWithLogitsBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def get_batch(split):\n",
    "    actions, labels = (train_actions, train_labels) if split == 'train' else (val_actions, val_labels)\n",
    "    # print(actions.shape)\n",
    "    actions = actions.view(100, -1, 8)\n",
    "    labels = labels.view(100, -1, 1)\n",
    "    # print(actions.view(100, -1, 8).shape)\n",
    "    # print(labels.view(100, -1, 1).shape)\n",
    "    random_offset = random.randint(0, len(actions) + 1)\n",
    "    if actions[random_offset:random_offset + 1].shape[0] == 0 or  labels[random_offset:random_offset + 1].shape[0] == 0:\n",
    "        return get_batch(split)\n",
    "    return actions[random_offset:random_offset + 1], labels[random_offset:random_offset + 1]\n",
    "\n",
    "\n",
    "max_iteration = 5\n",
    "block_size = 8\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
    "\n",
    "for _ in range(max_iteration):\n",
    "    # Get Samples\n",
    "    actions, labels = get_batch('train')\n",
    "\n",
    "    # Evaluate the loss\n",
    "    logits, loss = model(actions, labels)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(loss)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:01:20.227835Z",
     "start_time": "2024-03-06T10:01:20.214273Z"
    }
   },
   "id": "6784fa88ad10c0ae",
   "execution_count": 2051
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(tensor([[[-3.4182]]], grad_fn=<ViewBackward0>), None)"
     },
     "execution_count": 2055,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction = model(torch.tensor([[[0., 1., 4., 5., 3., 5., 3., 5.]]]))\n",
    "prediction"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T10:01:21.916018Z",
     "start_time": "2024-03-06T10:01:21.911065Z"
    }
   },
   "id": "6d86270de299b7b0",
   "execution_count": 2055
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "actions, labels = get_batch('train')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T09:43:10.403171Z",
     "start_time": "2024-03-06T09:43:10.399592Z"
    }
   },
   "id": "cc29ff91137d6203",
   "execution_count": 1677
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([1, 30, 16])"
     },
     "execution_count": 1773,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Version 4 Self-Attention\n",
    "import math\n",
    "\n",
    "torch.manual_seed(1337)\n",
    "B, T, C = 4, 4, 32\n",
    "# x = torch.randn(B, T, C)\n",
    "x = actions\n",
    "C = 8\n",
    "\n",
    "head_size = 16\n",
    "key = nn.Linear(C, head_size, bias=False)\n",
    "query = nn.Linear(C, head_size, bias=False)\n",
    "value = nn.Linear(C, head_size, bias=False)\n",
    "\n",
    "k = key(x)  # (B, T, 16)\n",
    "q = query(x)  # (B, T, 16)\n",
    "v = value(x)  # (B, T, 16)\n",
    "wei = q @ k.transpose(-2, -1) * (1 / math.sqrt(k.size(-1)))  # (B, T, 16) @ (B, 16, T) = (B, T, T)\n",
    "# tril = torch.tril(torch.ones((T, T)))\n",
    "# wei = wei.masked_fill(tril == 0, float('-inf'))\n",
    "wei = F.softmax(wei, dim=-1)\n",
    "out = wei @ v  # (B, T, T) @ (B, T, 16) = (B, T, 16)\n",
    "\n",
    "out.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T09:49:11.168727Z",
     "start_time": "2024-03-06T09:49:11.160997Z"
    }
   },
   "id": "d3163e784ba88269",
   "execution_count": 1773
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
